{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Este código foi baseado em https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi"
      ],
      "metadata": {
        "id": "uBpX5XegnRvr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ORGANIZAÇÃO DO DATASET"
      ],
      "metadata": {
        "id": "_Mj8TlpcQ3zH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para treinar Ireneo, iniciamos clonando os modelos do Tensorflow no Github, uma vez que utilizaremos um modelo pré-treinado para a realização de Transfer Learning"
      ],
      "metadata": {
        "id": "rs0VncVdnVP3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3L2IFgD3HJL"
      },
      "outputs": [],
      "source": [
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos o comando protoc para compilar arquivos .proto presentes no diretório de detecção de objetos do Tensorflow"
      ],
      "metadata": {
        "id": "GAacDHXLRLJg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QPmVBSlLTzM"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modificamos o arquivo setup.py para ser direcionado à uma versão adequada dos modelos TensorFlow"
      ],
      "metadata": {
        "id": "4Tmv53cfIHtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "with open('/content/models/research/object_detection/packages/tf2/setup.py') as f:\n",
        "    s = f.read()\n",
        "\n",
        "with open('/content/models/research/setup.py', 'w') as f:\n",
        "    # reescrevemos a versão dos modelos\n",
        "    s = re.sub('tf-models-official>=2.5.1',\n",
        "               'tf-models-official==2.8.0', s)\n",
        "    f.write(s)"
      ],
      "metadata": {
        "id": "NRBnuCKjM4Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalações diversas necessárias para o treinamento"
      ],
      "metadata": {
        "id": "RN3IFZ4MJK6j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLDnCkLLwLr6"
      },
      "outputs": [],
      "source": [
        "!pip install pyyaml==5.3\n",
        "!pip install /content/models/research/\n",
        "!pip install tensorflow==2.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execução de um script de testes dos modelos instalados, para verificar se está tudo funcional"
      ],
      "metadata": {
        "id": "d4Dd90NgJOCR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh_HPMOqWH9z"
      },
      "outputs": [],
      "source": [
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criamos um diretório para depois extrair do .zip e armazenar as imagens e anotações do dataset de treinamento. Nesse momento, deve existir um arquivo images.zip contendo todas as imagens e anotações necessárias (esse arquivo é gerado a partir do *Label Studio*)."
      ],
      "metadata": {
        "id": "HXUDWn5WJbcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir content"
      ],
      "metadata": {
        "id": "t_cZ0Pcl7x-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q images.zip -d /content/images/"
      ],
      "metadata": {
        "id": "GzIjNo6976Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baixamos scripts auxiliares e os executamos para fazer a conversão das anotações no formato VOC para CSV, separando também as imagens e anotações em conjuntos de treinamento e validação"
      ],
      "metadata": {
        "id": "A3o8YgHcJ1hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://raw.githubusercontent.com/svenancio/ireneo/refs/heads/main/src/treinamento/create_csv.py\n",
        "! wget https://raw.githubusercontent.com/svenancio/ireneo/refs/heads/main/src/treinamento/create_tfrecord.py"
      ],
      "metadata": {
        "id": "CN7paqSzQqa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tdDbTmHYwu-"
      },
      "outputs": [],
      "source": [
        "# Create CSV data files and TFRecord files\n",
        "!python3 create_csv.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertemos os dados anotados de imagens em arquivos TFRecord, um formato binário eficiente usado pelo TensorFlow para carregar grandes quantidades de dados de treinamento. Ele lê anotações das imagens (como coordenadas de *bounding boxes* e rótulos de objetos) e converte cada imagem e suas anotações em um exemplo TFRecord, usando uma estrutura de dados personalizada para organizar as informações. O script também gera um arquivo labelmap.pbtxt que associa classes de objetos a identificadores numéricos, necessário para treinar modelos de detecção de objetos com o TensorFlow."
      ],
      "metadata": {
        "id": "wvP6kWQtSmYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 create_tfrecord.py --csv_input=images/train_labels.csv --labelmap=labelmap.txt --image_dir=images/train --output_path=train.tfrecord\n",
        "!python3 create_tfrecord.py --csv_input=images/validation_labels.csv --labelmap=labelmap.txt --image_dir=images/validation --output_path=val.tfrecord"
      ],
      "metadata": {
        "id": "dCYoY4wB9Ump"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criamos variáveis para armazenar os caminhos e referências dos"
      ],
      "metadata": {
        "id": "0ik9JjA1Rxud"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUd2wtfrqedy"
      },
      "outputs": [],
      "source": [
        "train_record_fname = '/content/train.tfrecord'\n",
        "val_record_fname = '/content/val.tfrecord'\n",
        "label_map_pbtxt_fname = '/content/labelmap.pbtxt'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TREINAMENTO DO MODELO"
      ],
      "metadata": {
        "id": "Z-sNf-2hnm-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temos agora à disposição diferentes modelos de detecção de objetos em imagens. Escolhemos um deles, mas deixamos os demais à disposição."
      ],
      "metadata": {
        "id": "Pq1BZG0mS9Je"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN0EUEa3e5Un"
      },
      "outputs": [],
      "source": [
        "MODELS_CONFIG = {\n",
        "    'ssd-mobilenet-v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    'efficientdet-d0': {\n",
        "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
        "    },\n",
        "    'ssd-mobilenet-v2-fpnlite-320': {\n",
        "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n",
        "    }\n",
        "}\n",
        "\n",
        "# Se necessário, escolha outro modelo a partir da lista acima\n",
        "chosen_model = 'ssd-mobilenet-v2-fpnlite-320'\n",
        "\n",
        "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
        "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
        "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criamos um diretório \"mymodel\" para armazenar os modelos pré-treinados, então os baixamos."
      ],
      "metadata": {
        "id": "-_CsnZGTY_kk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG4TmJUVrYQ7"
      },
      "outputs": [],
      "source": [
        "%mkdir /content/models/mymodel/\n",
        "%cd /content/models/mymodel/\n",
        "\n",
        "# Faz o download do modelo pré-treinado escolhido, em termo dos pesos de suas redes neurais\n",
        "import tarfile\n",
        "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
        "!wget {download_tar}\n",
        "tar = tarfile.open(pretrained_checkpoint)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "# download do arquivo de configuração do modelo pré-treinado escolhido\n",
        "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
        "!wget {download_config}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuramos os parâmetros de treinamento, como número de passos (quantidade de vezes que o modelo ajusta seus pesos) e tamanho do \"batch\" (Cada batch é composto por várias imagens, incluindo suas respectivas anotações, como coordenadas de caixas delimitadoras e classes de objetos. Durante o treinamento, o modelo processa um batch por vez e ajusta seus pesos com base no erro médio calculado sobre todas as amostras no batch)"
      ],
      "metadata": {
        "id": "X0pRU8n9ZikT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lYDvJN-n69v"
      },
      "outputs": [],
      "source": [
        "num_steps = 40000\n",
        "\n",
        "if chosen_model == 'efficientdet-d0':\n",
        "  batch_size = 4\n",
        "else:\n",
        "  batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuramos mais alguns diretórios para receber arquivos de configuração e checkpoints de treinamento. Então extraímos o número de classes definidas no dataset de treinamento"
      ],
      "metadata": {
        "id": "txtJKJH7bGcI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_ki9jOqxn7V"
      },
      "outputs": [],
      "source": [
        "pipeline_fname = '/content/models/mymodel/' + base_pipeline_file\n",
        "fine_tune_checkpoint = '/content/models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "print('Total de classes:', num_classes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criamos um arquivo de configuração customizado para o treinamento de um modelo de detecção de objetos, ajustando parâmetros específicos no arquivo de pipeline base. Primeiro, carregamos o conteúdo do arquivo base de configuração (pipeline_fname) e, usando expressões regulares, substituimos valores como o caminho do checkpoint de pré-treinamento, os arquivos TFRecord de treino e teste, o caminho do arquivo de mapeamento de classes (label_map_path), o tamanho do batch (batch_size), o número de passos de treino (num_steps) e o número de classes (num_classes). Dependendo do modelo escolhido, também fazemos ajustes específicos. Ao final, escrevemos as alterações em um novo arquivo de configuração (pipeline_file.config)."
      ],
      "metadata": {
        "id": "kN_j9NqxbrZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eA5ht3_yukT"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "%cd /content/models/mymodel\n",
        "print('escrevendo arquivo de configuração customizado')\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "\n",
        "with open('pipeline_file.config', 'w') as f:\n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n",
        "\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "\n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "\n",
        "    s = re.sub(\n",
        "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "\n",
        "    if chosen_model == 'ssd-mobilenet-v2':\n",
        "      s = re.sub('learning_rate_base: .8',\n",
        "                 'learning_rate_base: .08', s)\n",
        "\n",
        "      s = re.sub('warmup_learning_rate: 0.13333',\n",
        "                 'warmup_learning_rate: .026666', s)\n",
        "\n",
        "    if chosen_model == 'efficientdet-d0':\n",
        "      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n",
        "      s = re.sub('pad_to_max_dimension: true', '', s)\n",
        "      s = re.sub('min_dimension', 'height', s)\n",
        "      s = re.sub('max_dimension', 'width', s)\n",
        "\n",
        "    f.write(s)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificamos uma última vez o arquivo de configuração customizado para verificar se está tudo correto"
      ],
      "metadata": {
        "id": "kOeXwFDbcyk_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEsOLOMHzBqF"
      },
      "outputs": [],
      "source": [
        "!cat /content/models/mymodel/pipeline_file.config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuramos mais algumas variáveis para armazenar caminhos e facilitar a referência"
      ],
      "metadata": {
        "id": "L1soBFWXc8dN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMlaN3rs3zLe"
      },
      "outputs": [],
      "source": [
        "# Set the path to the custom config file and the directory to store training checkpoints in\n",
        "pipeline_file = '/content/models/mymodel/pipeline_file.config'\n",
        "model_dir = '/content/training/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Carregamos neste momento um Tensorboard, que nos ajuda a acompanhar o processo de treinamento"
      ],
      "metadata": {
        "id": "5S2fGG4AdMeU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TI9iCCxoNlAL"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/training/train'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXECUÇÃO DO TREINAMENTO POR TRANSFERÊNCIA (TRANSFER LEARNING)!**\n",
        "\n",
        "Hora de tomar um café ou fazer uns alongamentos"
      ],
      "metadata": {
        "id": "TGfgVRXidXsV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQTfZChVzzpZ"
      },
      "outputs": [],
      "source": [
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_file} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quando terminar o treinamento, armazenamos o modelo treinado em outro diretório"
      ],
      "metadata": {
        "id": "27N-QgaadwCh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaUU8tBlHifd"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/custom_model_lite\n",
        "output_directory = '/content/custom_model_lite'\n",
        "\n",
        "last_model_path = '/content/training'\n",
        "\n",
        "!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "    --trained_checkpoint_dir {last_model_path} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_file}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversão de um modelo TensorFlow salvo em um arquivo de modelo otimizado para dispositivos móveis e embarcados no formato TFLite (TensorFlow Lite)."
      ],
      "metadata": {
        "id": "i7JY1aQteMlN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsE_uVjlsz3u"
      },
      "outputs": [],
      "source": [
        "# Convert exported graph file into TFLite model file\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('/content/custom_model_lite/detect.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CeoLXOI4m6W8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TESTE E ANÁLISE DO MODELO"
      ],
      "metadata": {
        "id": "ir9MK4IkeTqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos uma função para detectar objetos em fotos separadas para treinamento, e comparar os resultados com os rótulos e \"bounding boxes\" cadastrados"
      ],
      "metadata": {
        "id": "0clPws-unacr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4WtI8i5K96w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import importlib.util\n",
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n",
        "\n",
        "  # Pega os nomes de arquivos de todas as imagens na pasta de teste\n",
        "  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
        "\n",
        "  # Carrega o mapa de rótulos na memória\n",
        "  with open(lblpath, 'r') as f:\n",
        "      labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  # Carrega o modelo TensorFlow Lite na memória\n",
        "  interpreter = Interpreter(model_path=modelpath)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  # Obtem detalhes do modelo\n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  height = input_details[0]['shape'][1]\n",
        "  width = input_details[0]['shape'][2]\n",
        "\n",
        "  float_input = (input_details[0]['dtype'] == np.float32)\n",
        "\n",
        "  input_mean = 127.5\n",
        "  input_std = 127.5\n",
        "\n",
        "  # Seleciona aleatoriamente as imagens de teste\n",
        "  images_to_test = random.sample(images, num_test_images)\n",
        "\n",
        "  # Faz a detecção para cada imagem\n",
        "  for image_path in images_to_test:\n",
        "\n",
        "      # Carrega a imagem e redimensiona para o formato esperado [1xHxWx3]\n",
        "      image = cv2.imread(image_path)\n",
        "      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      imH, imW, _ = image.shape\n",
        "      image_resized = cv2.resize(image_rgb, (width, height))\n",
        "      input_data = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "      # Normaliza os valores de pixel se estiver usando um modelo de ponto flutuante (não quantizado)\n",
        "      if float_input:\n",
        "          input_data = (np.float32(input_data) - input_mean) / input_std\n",
        "\n",
        "      # Realiza a detecção executando o modelo com a imagem como entrada\n",
        "      interpreter.set_tensor(input_details[0]['index'],input_data)\n",
        "      interpreter.invoke()\n",
        "\n",
        "      # Recupera os resultados da detecção\n",
        "      boxes = interpreter.get_tensor(output_details[1]['index'])[0] # Coordenadas da Bounding box dos objetos detectados\n",
        "      classes = interpreter.get_tensor(output_details[3]['index'])[0] # índices das classes dos objetos detectados\n",
        "      scores = interpreter.get_tensor(output_details[0]['index'])[0] # nível de confiança de objetos detectados\n",
        "\n",
        "      detections = []\n",
        "\n",
        "      # Passa por todas as detecções e desenha a caixa de detecção se a confiança estiver acima do limite mínimo\n",
        "      for i in range(len(scores)):\n",
        "          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
        "\n",
        "              # Obtém as coordenadas da caixa delimitadora e desenha a caixa\n",
        "              # O \"interpreter\" pode retornar coordenadas fora das dimensões da imagem; é necessário ajustá-las usando max() e min()\n",
        "              ymin = int(max(1,(boxes[i][0] * imH)))\n",
        "              xmin = int(max(1,(boxes[i][1] * imW)))\n",
        "              ymax = int(min(imH,(boxes[i][2] * imH)))\n",
        "              xmax = int(min(imW,(boxes[i][3] * imW)))\n",
        "\n",
        "              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
        "\n",
        "              # Desenha o rótulo\n",
        "              object_name = labels[int(classes[i])] # Look up object name from \"labels\" array using class index\n",
        "              label = '%s: %d%%' % (object_name, int(scores[i]*100)) # Example: 'person: 72%'\n",
        "              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) # Get font size\n",
        "              label_ymin = max(ymin, labelSize[1] + 10) # Make sure not to draw label too close to top of window\n",
        "              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) # Draw white box to put label text in\n",
        "              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) # Draw label text\n",
        "\n",
        "              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
        "\n",
        "\n",
        "      # Todos os resultados foram desenhados na imagem, agora exibe a imagem\n",
        "      if txt_only == False: # \"text_only\" controla se queremos exibir os resultados da imagem ou apenas salvá-los em arquivos .txt\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        plt.figure(figsize=(12,16))\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "\n",
        "      # Salva os resultados da detecção em arquivos .txt (para calcular mAP)\n",
        "      elif txt_only == True:\n",
        "\n",
        "        # Obtém os nomes dos arquivos e caminhos\n",
        "        image_fn = os.path.basename(image_path)\n",
        "        base_fn, ext = os.path.splitext(image_fn)\n",
        "        txt_result_fn = base_fn +'.txt'\n",
        "        txt_savepath = os.path.join(savepath, txt_result_fn)\n",
        "\n",
        "        # Escreve os resultados no arquivo de texto\n",
        "        # (Usando o formato definido em https://github.com/Cartucho/mAP, que facilita o cálculo de mAP)\n",
        "        with open(txt_savepath,'w') as f:\n",
        "            for detection in detections:\n",
        "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executa a função de detectar objetos nas imagens de teste"
      ],
      "metadata": {
        "id": "dzHDjZJogtT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_TO_IMAGES='/content/images/test'\n",
        "PATH_TO_MODEL='/content/custom_model_lite/detect.tflite'\n",
        "PATH_TO_LABELS='/content/labelmap.txt'\n",
        "min_conf_threshold=0.5   # limiar de confiança (modifique para 0.01 se você não ver nenhum resultado de detecção)\n",
        "images_to_test = 10   # quantidade de imagens a serem detectadas\n",
        "\n",
        "# executa a detecção de objetos\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
      ],
      "metadata": {
        "id": "6t8CMarqBqP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verificação do nível geral de precisão do modelo\n",
        "\n",
        "Usamos bibliotecas externas para calcular o índice mAP (mean Average Precision)"
      ],
      "metadata": {
        "id": "ubJFXr1ejHxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/Cartucho/mAP /content/mAP\n",
        "cd /content/mAP\n",
        "rm input/detection-results/*\n",
        "rm input/ground-truth/*\n",
        "rm input/images-optional/*\n",
        "wget https://raw.githubusercontent.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi/master/util_scripts/calculate_map_cartucho.py"
      ],
      "metadata": {
        "id": "JlWarXEZDUqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/images/test/* /content/mAP/input/images-optional # Copy images and xml files\n",
        "!mv /content/mAP/input/images-optional/*.xml /content/mAP/input/ground-truth/  # Move xml files to the appropriate folder"
      ],
      "metadata": {
        "id": "5szFfVxwI3wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/mAP/scripts/extra/convert_gt_xml.py"
      ],
      "metadata": {
        "id": "qdjtOUDnK2AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configura novamente variáveis para rodar a inferência, desta vez para salvar os resultados da detecção como arquivos .txt\n",
        "PATH_TO_IMAGES='/content/images/test'   # Caminho para a pasta de imagens de teste\n",
        "PATH_TO_MODEL='/content/custom_model_lite/detect.tflite'   # Caminho para o arquivo de modelo .tflite\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   # Caminho para o arquivo labelmap.txt\n",
        "PATH_TO_RESULTS='/content/mAP/input/detection-results' # Pasta para salvar os resultados da detecção\n",
        "min_conf_threshold=0.1   # Limite mínimo de confiança\n",
        "\n",
        "# Usa todas as imagens na pasta de teste\n",
        "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
        "images_to_test = len(image_list)\n",
        "\n",
        "# Diz à função para apenas salvar os resultados e não exibir as imagens\n",
        "txt_only = True\n",
        "\n",
        "# Executamos novamente\n",
        "print('Iniciando inferência em %d imagens...' % images_to_test)\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
        "print('Inferência finalizada!')"
      ],
      "metadata": {
        "id": "szzHFAhsMNFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Executamos a análise do mAP sobre todos os rótulos do modelo"
      ],
      "metadata": {
        "id": "Jjam6KnimqNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mAP\n",
        "!python calculate_map_cartucho.py --labels=/content/labelmap.txt"
      ],
      "metadata": {
        "id": "3DkjpIBARTQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COPIANDO O MODELO TREINADO"
      ],
      "metadata": {
        "id": "rMnZB8jQ40bg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reunimos na mesma pasta os arquivos labelmap.txt .pbtxt e a configuração junto ao modelo, e criamos um arquivo .zip"
      ],
      "metadata": {
        "id": "PEJtM-CXmM70"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awZMQGVqMpVL"
      },
      "outputs": [],
      "source": [
        "# Move labelmap and pipeline config files into TFLite model folder and zip it up\n",
        "!cp /content/labelmap.txt /content/custom_model_lite\n",
        "!cp /content/labelmap.pbtxt /content/custom_model_lite\n",
        "!cp /content/models/mymodel/pipeline_file.config /content/custom_model_lite\n",
        "\n",
        "%cd /content\n",
        "!zip -r custom_model_lite.zip custom_model_lite"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente obtemos o modelo treinado, que será embarcado no dispositivo discriminativo Ireneo"
      ],
      "metadata": {
        "id": "CTEov2sOl-I8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVPfAGbNPV56"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/custom_model_lite.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantização do modelo"
      ],
      "metadata": {
        "id": "kYhZokJc6Flt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSNZtfj_k3NP"
      },
      "outputs": [],
      "source": [
        "# Get list of all images in train directory\n",
        "image_path = '/content/images/train'\n",
        "\n",
        "jpg_file_list = glob.glob(image_path + '/*.jpg')\n",
        "JPG_file_list = glob.glob(image_path + '/*.JPG')\n",
        "png_file_list = glob.glob(image_path + '/*.png')\n",
        "bmp_file_list = glob.glob(image_path + '/*.bmp')\n",
        "\n",
        "quant_image_list = jpg_file_list + JPG_file_list + png_file_list + bmp_file_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORzx0XRErSLV"
      },
      "outputs": [],
      "source": [
        "# A generator that provides a representative dataset\n",
        "# Code modified from https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\n",
        "\n",
        "# First, get input details for model so we know how to preprocess images\n",
        "interpreter = Interpreter(model_path=PATH_TO_MODEL) # PATH_TO_MODEL is defined in Step 7 above\n",
        "interpreter.allocate_tensors()\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "height = input_details[0]['shape'][1]\n",
        "width = input_details[0]['shape'][2]\n",
        "\n",
        "import random\n",
        "\n",
        "def representative_data_gen():\n",
        "  dataset_list = quant_image_list\n",
        "  quant_num = 300\n",
        "  for i in range(quant_num):\n",
        "    pick_me = random.choice(dataset_list)\n",
        "    image = tf.io.read_file(pick_me)\n",
        "\n",
        "    if pick_me.endswith('.jpg') or pick_me.endswith('.JPG'):\n",
        "      image = tf.io.decode_jpeg(image, channels=3)\n",
        "    elif pick_me.endswith('.png'):\n",
        "      image = tf.io.decode_png(image, channels=3)\n",
        "    elif pick_me.endswith('.bmp'):\n",
        "      image = tf.io.decode_bmp(image, channels=3)\n",
        "\n",
        "    image = tf.image.resize(image, [width, height])  # TO DO: Replace 300s with an automatic way of reading network input size\n",
        "    image = tf.cast(image / 255., tf.float32)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    yield [image]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ox0bGDWds_Ce"
      },
      "outputs": [],
      "source": [
        "# Initialize converter module\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n",
        "\n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input tensors to uint8 and output tensors to float32\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.float32\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('/content/custom_model_lite/detect_quant.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up parameters for inferencing function (using detect_quant.tflite instead of detect.tflite)\n",
        "PATH_TO_IMAGES='/content/images/test'   #Path to test images folder\n",
        "PATH_TO_MODEL='/content/custom_model_lite/detect_quant.tflite'   #Path to .tflite model file\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   #Path to labelmap.txt file\n",
        "min_conf_threshold=0.5   #Confidence threshold (try changing this to 0.01 if you don't see any detection results)\n",
        "images_to_test = 10   #Number of images to run detection on\n",
        "\n",
        "# Run inferencing function!\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
      ],
      "metadata": {
        "id": "6OoirJuOtdOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Need to remove existing detection results first\n",
        "!rm /content/mAP/input/detection-results/*\n",
        "\n",
        "# Set up variables for running inference, this time to get detection results saved as .txt files\n",
        "PATH_TO_IMAGES='/content/images/test'   # Path to test images folder\n",
        "PATH_TO_MODEL='/content/custom_model_lite/detect_quant.tflite'   # Path to quantized .tflite model file\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   # Path to labelmap.txt file\n",
        "PATH_TO_RESULTS='/content/mAP/input/detection-results' # Folder to save detection results in\n",
        "min_conf_threshold=0.1   # Confidence threshold\n",
        "\n",
        "# Use all the images in the test folder\n",
        "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
        "images_to_test = min(500, len(image_list)) # If there are more than 500 images in the folder, just use 500\n",
        "\n",
        "# Tell function to just save results and not display images\n",
        "txt_only = True\n",
        "\n",
        "# Run inferencing function!\n",
        "print('Starting inference on %d images...' % images_to_test)\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
        "print('Finished inferencing!')"
      ],
      "metadata": {
        "id": "ZMaumV-11Et0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1834f787-b1c6-45f6-c83a-a96ce2931f01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting inference on 85 images...\n",
            "Finished inferencing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/mAP"
      ],
      "metadata": {
        "id": "ZIRNp0Af1Et1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content"
      ],
      "metadata": {
        "id": "1Mt1oMUd8wuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/mAP/outputs/"
      ],
      "metadata": {
        "id": "Su5C-Enb9Os_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python calculate_map_cartucho.py --labels=/content/labelmap.txt"
      ],
      "metadata": {
        "id": "4TDgMBw_1Et1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}